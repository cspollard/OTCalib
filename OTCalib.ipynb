{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OTCalib.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMCRg85sBUFDyxh3nHjkAx4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC2NF4bZLMlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "device='cuda'\n",
        "\n",
        "\n",
        "def poly(cs, xs):\n",
        "  ys = torch.zeros_like(xs)\n",
        "  for (i, c) in enumerate(cs):\n",
        "    ys += c * xs**i\n",
        "\n",
        "  return ys\n",
        "\n",
        "\n",
        "def discMC(xs, logpts):\n",
        "  return poly([0, 0.6, 1.2, 1.1], xs) * poly([1, 0.1, -0.01], logpts)\n",
        "\n",
        "def discData(xs, logpts):\n",
        "  return poly([0, 0.7, 1.1, 1.3], xs) * poly([1, -0.1, 0.02], logpts)\n",
        "\n",
        "\n",
        "def logptMC(xs):\n",
        "  return torch.log(poly([25, 50, 7], -torch.log(xs)))\n",
        "\n",
        "def logptData(xs):\n",
        "  return torch.log(poly([25, 45, 5], -torch.log(xs)))\n",
        "\n",
        "\n",
        "# need to add a small number to avoid values of zero.\n",
        "def genMC(n):\n",
        "  xs = torch.rand(n, device=device) + 1e-5\n",
        "  logpts = logptMC(xs)\n",
        "  ys = torch.rand(n, device=device) + 1e-5\n",
        "  ds = discMC(ys, logpts)\n",
        "  return torch.stack([logpts, ds]).transpose(0, 1)\n",
        "\n",
        "def genData(n):\n",
        "  xs = torch.rand(n, device=device) + 1e-5\n",
        "  logpts = logptData(xs)\n",
        "  ys = torch.rand(n, device=device) + 1e-5\n",
        "  ds = discData(ys, logpts)\n",
        "  return torch.stack([logpts, ds]).transpose(0, 1)\n",
        "\n",
        "\n",
        "# TEST\n",
        "\n",
        "\n",
        "def test(n):\n",
        "  mc = genMC(n).cpu().numpy()\n",
        "  data = genData(n).cpu().numpy()\n",
        "\n",
        "  plt.figure(figsize=(30, 10))\n",
        "\n",
        "  plt.subplot(1, 3, 1)\n",
        "\n",
        "  _ = plt.hist([np.exp(mc[:,0]), np.exp(data[:,0])], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"pT\")\n",
        "  plt.yscale(\"log\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "\n",
        "\n",
        "  _ = plt.hist([mc[:,0], data[:,0]], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"log pT\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "\n",
        "  _ = plt.hist([mc[:,1], data[:,1]], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"discriminant\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "test(int(1e6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDPdBAp8R3RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RacvVN4HTDdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.autograd as autograd\n",
        "\n",
        "\n",
        "# this worked well.\n",
        "# transport = \\\n",
        "#   nn.Sequential(\n",
        "#     nn.Linear(2, 256)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(256, 256)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(256, 1)\n",
        "#   )\n",
        "\n",
        "\n",
        "# adversary = \\\n",
        "#   nn.Sequential(\n",
        "#     nn.Linear(2, 256)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(256, 256)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(256, 1)\n",
        "#   )\n",
        "\n",
        "# tlr = 1e-7\n",
        "# alr = 5e-6\n",
        "\n",
        "\n",
        "transport = \\\n",
        "  nn.Sequential(\n",
        "    nn.Linear(2, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 1)\n",
        "  )\n",
        "\n",
        "\n",
        "adversary = \\\n",
        "  nn.Sequential(\n",
        "    nn.Linear(2, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 1)\n",
        "  )\n",
        "\n",
        "\n",
        "tlr = 1e-7\n",
        "alr = 5e-6\n",
        "\n",
        "\n",
        "transport.to(device)\n",
        "adversary.to(device)\n",
        "\n",
        "\n",
        "def tloss(xs):\n",
        "  return torch.mean(xs**2)\n",
        "\n",
        "aloss = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "\n",
        "toptim = torch.optim.Adam(transport.parameters(), lr=tlr)\n",
        "aoptim = torch.optim.Adam(adversary.parameters(), lr=alr)\n",
        "\n",
        "\n",
        "transport"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuuYHqdlNKu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import log\n",
        "import torch.utils.data as d\n",
        "\n",
        "def tonp(xs):\n",
        "  return xs.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "lam = 500\n",
        "\n",
        "\n",
        "# build validation samples at 25 and 500 GeV\n",
        "nval = int(2**20)\n",
        "\n",
        "pt25 = torch.ones(nval, device=device)*log(25)\n",
        "pt500 = torch.ones(nval, device=device)*log(500)\n",
        "\n",
        "valxs = torch.rand(nval, device=device) + 1e-3\n",
        "datads_pt25 = tonp(discData(valxs, pt25))\n",
        "datads_pt500 = tonp(discData(valxs, pt500))\n",
        "\n",
        "valxs = torch.rand(nval, device=device) + 1e-3\n",
        "mcds_pt25 = discMC(valxs, pt25)\n",
        "mcds_pt500 = discMC(valxs, pt500)\n",
        "\n",
        "histmc_pt25 = torch.stack([pt25, mcds_pt25]).transpose(0, 1)\n",
        "histmc_pt500 = torch.stack([pt500, mcds_pt500]).transpose(0, 1)\n",
        "\n",
        "mcds_pt25 = tonp(mcds_pt25)\n",
        "mcds_pt500 = tonp(mcds_pt500)\n",
        "\n",
        "\n",
        "nepochs = int(1e6)\n",
        "batchsize = 2**10\n",
        "\n",
        "datasize = int(2**20)\n",
        "\n",
        "alldata = genData(datasize)\n",
        "allmc = genMC(datasize)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "nbatches = datasize // batchsize\n",
        "\n",
        "# with autograd.detect_anomaly():\n",
        "for epoch in range(nepochs):\n",
        "  radvloss = 0\n",
        "  fadvloss = 0\n",
        "  tadvloss = 0\n",
        "  ttransloss = 0\n",
        "  realavg = 0\n",
        "  fakeavg = 0\n",
        "\n",
        "  for batch in range(nbatches):\n",
        "    data = alldata[torch.randint(alldata.size()[0], size=(batchsize,))]\n",
        "    mc = allmc[torch.randint(allmc.size()[0], size=(batchsize,))]\n",
        "\n",
        "\n",
        "    toptim.zero_grad()\n",
        "    aoptim.zero_grad()\n",
        "\n",
        "    real = adversary(data)\n",
        "\n",
        "    transporting = transport(mc)\n",
        "    transported = transporting + mc[:,1:]\n",
        "    fake = adversary(torch.cat([mc[:,:1], transported], axis=1))\n",
        "\n",
        "    realavg += torch.mean(real).item()\n",
        "    fakeavg += torch.mean(fake).item()\n",
        "    \n",
        "    tmp1 = aloss(real, torch.ones_like(real))\n",
        "    radvloss += tmp1.item()\n",
        "\n",
        "    tmp2 = aloss(fake, torch.zeros_like(fake))\n",
        "    fadvloss += tmp1.item()\n",
        "\n",
        "    loss = tmp1 + tmp2\n",
        "\n",
        "    loss.backward()\n",
        "    aoptim.step()\n",
        "\n",
        "\n",
        "    toptim.zero_grad()\n",
        "    aoptim.zero_grad()\n",
        "\n",
        "    transporting = transport(mc)\n",
        "    transported = transporting + mc[:,1:]\n",
        "    fake = adversary(torch.cat([mc[:,:1], transported], axis=1))\n",
        "\n",
        "    tmp1 = tloss(transporting)\n",
        "    ttransloss += tmp1.item()\n",
        "\n",
        "    tmp2 = lam * aloss(fake, torch.ones_like(fake))\n",
        "    tadvloss += tmp2.item()\n",
        "\n",
        "    loss = tmp1 + tmp2\n",
        "\n",
        "    loss.backward()\n",
        "    toptim.step()\n",
        "\n",
        "\n",
        "  # write tensorboard info once per epoch\n",
        "  writer.add_scalar('radvloss', radvloss / nbatches, epoch)\n",
        "  writer.add_scalar('fadvloss', fadvloss / nbatches, epoch)\n",
        "  writer.add_scalar('tadvloss', tadvloss / nbatches, epoch)\n",
        "  writer.add_scalar('ttransloss', ttransloss / nbatches, epoch)\n",
        "  writer.add_scalar('realavg', realavg / nbatches, epoch)\n",
        "  writer.add_scalar('fakeavg', fakeavg / nbatches, epoch)\n",
        "\n",
        "\n",
        "  # make validation plots once per epoch\n",
        "  transporting_pt25 = transport(histmc_pt25).squeeze()\n",
        "  transporting_pt500 = transport(histmc_pt500).squeeze()\n",
        "\n",
        "  transported_pt25 = tonp(transporting_pt25 + histmc_pt25[:,1])\n",
        "  transported_pt500 = tonp(transporting_pt500 + histmc_pt500[:,1])\n",
        "\n",
        "  transporting_pt25 = tonp(transporting_pt25)\n",
        "  transporting_pt500 = tonp(transporting_pt500)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(36, 6))\n",
        "\n",
        "  plt.subplot(1, 6, 1)\n",
        "\n",
        "  rangex = (0, 5)\n",
        "  rangey = (-0.75, 0.25)\n",
        "\n",
        "  h, b, _ = plt.hist( \\\n",
        "        [mcds_pt25, transported_pt25, datads_pt25]\n",
        "      , bins=25\n",
        "      , range=rangex\n",
        "      , density=True\n",
        "      , label=[\"mc\", \"transported\", \"data\"]\n",
        "      )\n",
        "  \n",
        "  plt.title(\"discriminant distribution, pT = 25\")\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 6, 2)\n",
        "\n",
        "\n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[0] - h[2]\n",
        "      , label=\"mc\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "  \n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[1] - h[2]\n",
        "      , label=\"transported\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "  \n",
        "  plt.ylim(-0.5, 0.5)\n",
        "  plt.title(\"discriminant difference to data, pT = 25\")\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.ylabel(\"prediction - data\")\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.subplot(1, 6, 3)\n",
        "\n",
        "  _ = plt.hist2d(mcds_pt25, transporting_pt25, range=[rangex, rangey], bins=20)\n",
        "  plt.title(\"discriminant transport, pT = 25\")\n",
        "  plt.xlabel(\"mc discriminant\")\n",
        "  plt.ylabel(\"transport vector\")\n",
        "\n",
        "\n",
        "  plt.subplot(1, 6, 4)\n",
        "\n",
        "  h, b, _ = plt.hist( \\\n",
        "        [mcds_pt500, transported_pt500, datads_pt500]\n",
        "      , bins=25\n",
        "      , range=rangex\n",
        "      , density=True\n",
        "      , label=[\"mc\", \"transported\", \"data\"]\n",
        "      )\n",
        "\n",
        "  plt.title(\"discriminant distribution, pT = 500\")\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.subplot(1, 6, 5)\n",
        "\n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[0] - h[2]\n",
        "      , label=\"mc\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "  \n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[1] - h[2]\n",
        "      , label=\"transported\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "  \n",
        "  plt.ylim(-0.5, 0.5)\n",
        "  plt.title(\"discriminant difference to data, pT = 500\")\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.ylabel(\"prediction - data\")\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.subplot(1, 6, 6)\n",
        "\n",
        "  _ = plt.hist2d(mcds_pt500, transporting_pt500, range=[rangex, rangey], bins=20)\n",
        "  plt.title(\"discriminant transport, pT = 500\")\n",
        "  plt.xlabel(\"mc discriminant\")\n",
        "  plt.ylabel(\"transport vector\")\n",
        "\n",
        "  plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vZ0s3jObznB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qldJqWitEJY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}