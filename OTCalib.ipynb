{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OTCalib.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNuSScOatyINEM9RhL68mFH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC2NF4bZLMlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from scipy.special import lambertw\n",
        "\n",
        "device='cuda'\n",
        "\n",
        "\n",
        "def poly(cs, xs):\n",
        "  ys = torch.zeros_like(xs)\n",
        "  for (i, c) in enumerate(cs):\n",
        "    ys += c * xs**i\n",
        "\n",
        "  return ys\n",
        "\n",
        "\n",
        "def discMC(xs, logpts):\n",
        "  return poly([0, 0.6, 1.2, 1.1], xs) * poly([1, 0.1, -0.01], logpts)\n",
        "\n",
        "def discData(xs, logpts):\n",
        "  return poly([0, 0.7, 1.1, 1.3], xs) * poly([1, -0.1, 0.02], logpts)\n",
        "\n",
        "\n",
        "def logptMC(xs):\n",
        "  return torch.log(poly([25, 50, 7], -torch.log(xs)))\n",
        "\n",
        "def logptData(xs):\n",
        "  return torch.log(poly([25, 45, 5], -torch.log(xs)))\n",
        "\n",
        "\n",
        "\n",
        "def bootstrap(n):\n",
        "  xs = np.random.rand(n)\n",
        "  ws = lambertw((xs-1)/np.e, k=-1).astype(np.float)\n",
        "  return - torch.from_numpy(ws).to(device) - 1\n",
        "\n",
        "\n",
        "# need to add a small number to avoid values of zero.\n",
        "def genMC(n):\n",
        "  xs = torch.rand(n, device=device) + 1e-5\n",
        "  logpts = logptMC(xs)\n",
        "  ys = torch.rand(n, device=device) + 1e-5\n",
        "  ds = discMC(ys, logpts)\n",
        "  return torch.stack([ds, logpts]).transpose(0, 1)\n",
        "\n",
        "def genData(n):\n",
        "  xs = torch.rand(n, device=device) + 1e-5\n",
        "  logpts = logptData(xs)\n",
        "  ys = torch.rand(n, device=device) + 1e-5\n",
        "  ds = discData(ys, logpts)\n",
        "  return torch.stack([ds, logpts]).transpose(0, 1)\n",
        "\n",
        "\n",
        "def test(n):\n",
        "  mc = genMC(n).cpu().numpy()\n",
        "  data = genData(n).cpu().numpy()\n",
        "\n",
        "  plt.figure(figsize=(30, 10))\n",
        "\n",
        "  plt.subplot(1, 3, 1)\n",
        "\n",
        "  _ = plt.hist([np.exp(mc[:,1]), np.exp(data[:,1])], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"pT\")\n",
        "  plt.yscale(\"log\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "\n",
        "\n",
        "  _ = plt.hist([mc[:,1], data[:,1]], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"log pT\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "\n",
        "  _ = plt.hist([mc[:,0], data[:,0]], bins=25, label=[\"mc\", \"data\"])\n",
        "  plt.title(\"discriminant\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "test(int(1e6))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDPdBAp8R3RU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RacvVN4HTDdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.autograd as autograd\n",
        "\n",
        "nps = 5\n",
        "\n",
        "\n",
        "# this worked well.\n",
        "# transport = \\\n",
        "#   nn.Sequential(\n",
        "#     nn.Linear(2, 512)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(512, 512)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(512, 1)\n",
        "#   )\n",
        "\n",
        "\n",
        "# adversary = \\\n",
        "#   nn.Sequential(\n",
        "#     nn.Linear(2, 512)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(512, 512)\n",
        "#   , nn.LeakyReLU(inplace=True)\n",
        "#   , nn.Linear(512, 1)\n",
        "#   )\n",
        "\n",
        "\n",
        "# tlr = 1e-7\n",
        "# alr = 5e-6\n",
        "\n",
        "\n",
        "transport = \\\n",
        "  nn.Sequential(\n",
        "    nn.Linear(2+nps, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 1)\n",
        "  )\n",
        "\n",
        "\n",
        "adversary = \\\n",
        "  nn.Sequential(\n",
        "    nn.Linear(2, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 512)\n",
        "  , nn.LeakyReLU(inplace=True)\n",
        "  , nn.Linear(512, 1)\n",
        "  )\n",
        "\n",
        "\n",
        "tlr = 1e-7\n",
        "alr = 5e-6\n",
        "\n",
        "\n",
        "transport.to(device)\n",
        "adversary.to(device)\n",
        "\n",
        "\n",
        "def tloss(xs):\n",
        "  return torch.mean(xs**2)\n",
        "\n",
        "\n",
        "# binary_cross_entropy_with_logits\n",
        "\n",
        "toptim = torch.optim.Adam(transport.parameters(), lr=tlr)\n",
        "aoptim = torch.optim.Adam(adversary.parameters(), lr=alr)\n",
        "\n",
        "\n",
        "transport"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuuYHqdlNKu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import exp, log\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "\n",
        "def tonp(xs):\n",
        "  return xs.cpu().detach().numpy()\n",
        "\n",
        "\n",
        "def plotPtTheta(logpt, toys):\n",
        "  zeros = torch.zeros((toys.size()[0], nps), device=device)\n",
        "  logpts = torch.ones(toys.size()[0], device=device)*logpt\n",
        "\n",
        "  data = torch.stack([torch.sort(discData(toys, logpts))[0], logpts]).transpose(0, 1)\n",
        "  mc = torch.stack([torch.sort(discMC(toys, logpts))[0], logpts]).transpose(0, 1)\n",
        "\n",
        "  thetas = zeros.clone()\n",
        "  transporting = transport(torch.cat([mc, thetas], axis=1))\n",
        "  nomtrans = tonp(transporting)\n",
        "  nom = tonp(transporting + mc[:,0:1])\n",
        "\n",
        "  postrans = []\n",
        "  negtrans = []\n",
        "  for i in range(nps):\n",
        "    thetas = zeros.clone()\n",
        "    thetas[:,i] = 1\n",
        "    transporting = transport(torch.cat([mc, thetas], axis=1))\n",
        "    postrans.append(tonp(transporting))\n",
        "\n",
        "    thetas = zeros.clone()\n",
        "    thetas[:,i] = -1\n",
        "    transporting = transport(torch.cat([mc, thetas], axis=1))\n",
        "    negtrans.append(tonp(transporting))\n",
        "\n",
        "\n",
        "  data = tonp(data)\n",
        "  mc = tonp(mc)\n",
        "\n",
        "  plt.figure(figsize=(18, 6))\n",
        "\n",
        "  plt.subplot(1, 3, 1)\n",
        "\n",
        "  rangex = (0, 5)\n",
        "  rangey = (-0.75, 0.25)\n",
        "\n",
        "  h, b, _ = plt.hist( \\\n",
        "        [mc[:,0], nom[:,0], data[:,0]]\n",
        "      , bins=25\n",
        "      , range=rangex\n",
        "      , density=True\n",
        "      , label=[\"mc\", \"nominal transported\", \"data\"]\n",
        "      )\n",
        "  \n",
        "  plt.title(\"discriminant distribution, (pT = %0.2f)\" % exp(logpt))\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 3, 2)\n",
        "\n",
        "\n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[0] - h[2]\n",
        "      , label=\"mc\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "\n",
        "  _ = plt.plot( \\\n",
        "        (b[:-1] + b[1:]) / 2.0\n",
        "      , h[1] - h[2]\n",
        "      , label=\"transported\"\n",
        "      , linewidth=3\n",
        "      )\n",
        "\n",
        "  plt.ylim(-0.5, 0.5)\n",
        "  plt.title(\"discriminant difference to data, (pT = %0.2f)\" % exp(logpt))\n",
        "  plt.xlabel(\"discriminant\")\n",
        "  plt.ylabel(\"prediction - data\")\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "  plt.subplot(1, 3, 3)\n",
        "\n",
        "\n",
        "  \n",
        "  cols = [\"blue\", \"green\", \"red\", \"orange\", \"magenta\"]\n",
        "  for i in range(nps):\n",
        "    _ = \\\n",
        "      plt.plot(\n",
        "          mc[:,0]\n",
        "        , postrans[i]\n",
        "        , c=cols[i]\n",
        "      )\n",
        "\n",
        "    _ = \\\n",
        "      plt.plot(\n",
        "          mc[:,0]\n",
        "        , negtrans[i]\n",
        "        , c=cols[i]\n",
        "      )\n",
        "\n",
        "\n",
        "  _ = \\\n",
        "    plt.plot(\n",
        "        mc[:,0]\n",
        "      , nomtrans\n",
        "      , c=\"black\"\n",
        "    )\n",
        "\n",
        "\n",
        "  \n",
        "  plt.xlim(rangex)\n",
        "  plt.ylim(rangey)\n",
        "  plt.title(\"discriminant transport, (pT = %0.2f)\" % exp(logpt))\n",
        "  plt.xlabel(\"mc discriminant\")\n",
        "  plt.ylabel(\"transport vector\")\n",
        "\n",
        "  plt.subplots_adjust(wspace=0.4)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "lam = 500\n",
        "\n",
        "\n",
        "# toys for validation samples\n",
        "nval = int(2**15)\n",
        "valtoys = torch.rand(nval, device=device)\n",
        "\n",
        "\n",
        "nepochs = 2**20\n",
        "batchsize = 2**12\n",
        "\n",
        "datasize = 2**20\n",
        "\n",
        "alldata = genData(datasize)\n",
        "allmc = genMC(datasize)\n",
        "\n",
        "writer = SummaryWriter()\n",
        "nbatches = datasize // batchsize\n",
        "\n",
        "\n",
        "# with autograd.detect_anomaly():\n",
        "for epoch in range(nepochs):\n",
        "  radvloss = 0\n",
        "  fadvloss = 0\n",
        "  tadvloss = 0\n",
        "  ttransloss = 0\n",
        "  realavg = 0\n",
        "  fakeavg = 0\n",
        "\n",
        "  for batch in range(nbatches):\n",
        "    straps = bootstrap(batchsize).unsqueeze(1)\n",
        "    thetas = torch.randn((batchsize, nps), device=device)\n",
        "\n",
        "\n",
        "    tmp = alldata[torch.randint(alldata.size()[0], size=(batchsize,), device=device)]\n",
        "    data = tmp\n",
        "\n",
        "    tmp = allmc[torch.randint(allmc.size()[0], size=(batchsize,), device=device)]\n",
        "    mc = tmp\n",
        "\n",
        "    toptim.zero_grad()\n",
        "    aoptim.zero_grad()\n",
        "\n",
        "    real = adversary(data)\n",
        "\n",
        "    transporting = transport(torch.cat([mc, thetas], axis=1))\n",
        "    transported = transporting + mc[:,0:1]\n",
        "    fake = adversary(torch.cat([transported, mc[:,1:]], axis=1))\n",
        "\n",
        "    realavg += torch.mean(real).item()\n",
        "    fakeavg += torch.mean(fake).item()\n",
        "\n",
        "    \n",
        "    tmp1 = \\\n",
        "      binary_cross_entropy_with_logits( \\\n",
        "          real\n",
        "        , torch.ones_like(real)\n",
        "        , weight=straps\n",
        "        , reduction='mean'\n",
        "        )\n",
        "\n",
        "    radvloss += tmp1.item()\n",
        "\n",
        "\n",
        "    tmp2 = \\\n",
        "      binary_cross_entropy_with_logits( \\\n",
        "          fake\n",
        "        , torch.zeros_like(real)\n",
        "        , reduction='mean'\n",
        "        )\n",
        "      \n",
        "    fadvloss += tmp2.item()\n",
        "\n",
        "    loss = tmp1 + tmp2\n",
        "\n",
        "    loss.backward()\n",
        "    aoptim.step()\n",
        "\n",
        "\n",
        "    toptim.zero_grad()\n",
        "    aoptim.zero_grad()\n",
        "\n",
        "    transporting = transport(torch.cat([mc, thetas], axis=1))\n",
        "    transported = transporting + mc[:,0:1]\n",
        "    fake = adversary(torch.cat([transported, mc[:,1:]], axis=1))\n",
        "\n",
        "    tmp1 = tloss(transporting)\n",
        "    ttransloss += tmp1.item()\n",
        "\n",
        "    tmp2 = \\\n",
        "      lam * \\\n",
        "      binary_cross_entropy_with_logits( \\\n",
        "          fake\n",
        "        , torch.ones_like(real)\n",
        "        , reduction='mean'\n",
        "        )\n",
        "      \n",
        "    tadvloss += tmp2.item()\n",
        "\n",
        "    loss = tmp2 # tmp1 + tmp2\n",
        "\n",
        "    loss.backward()\n",
        "    toptim.step()\n",
        "\n",
        "\n",
        "  # write tensorboard info once per epoch\n",
        "  writer.add_scalar('radvloss', radvloss / nbatches, epoch)\n",
        "  writer.add_scalar('fadvloss', fadvloss / nbatches, epoch)\n",
        "  writer.add_scalar('tadvloss', tadvloss / nbatches, epoch)\n",
        "  writer.add_scalar('ttransloss', ttransloss / nbatches, epoch)\n",
        "  writer.add_scalar('realavg', realavg / nbatches, epoch)\n",
        "  writer.add_scalar('fakeavg', fakeavg / nbatches, epoch)\n",
        "\n",
        "\n",
        "  # make validation plots once per epoch\n",
        "  plotPtTheta(log(25), valtoys)\n",
        "  plotPtTheta(log(250), valtoys)\n",
        "  plotPtTheta(log(500), valtoys)\n",
        "  plotPtTheta(log(1000), valtoys)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vZ0s3jObznB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qldJqWitEJY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}